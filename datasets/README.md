# Датасеты для моделинга OCR и детекции

## OCR

Всего для OCR модели сейчас используется 7 датасетов.

### Список использованных датасетов:

**Важно:** Все датасеты находятся в открытом доступе и не требуют специальной лицензии.

1) [DDI (DistortedDocumentImage-100)](https://github.com/machine-intelligence-laboratory/DDI-100/tree/master/dataset)
    
    Датасет, собранный из книг с различными аугментациями. 
    
    **Печатный. Русский язык. Сканы.** 
    
    Качество датасета выше среднего (иногда плохие сэмплы). Размер - 54к.
2) [GNHK](https://github.com/GoodNotes/GNHK-dataset)

    Фотографии заметок среднего качества, в виде in-wild текста. 
    
    **Рукописный. Английский язык. Фото.**
    
    Качество датасета выше среднего (иногда множество спецсимволов, их надо чистить через hnm). Размер - около 31к.
3) [IAT (IAM-line)](https://huggingface.co/datasets/Teklia/IAM-line)

    Датасет длинных предложений. 
    
    **Рукописный. Английский язык. Сканы.**
    
    Качество не самое лучшее (смотри заметку ниже)

    **Заметка:** данный датасет проходил предварительную эвристическую доразметку с помощью скрипта вырезания отдельных слов, поэтому имеет проблемы с разметкой. Надо прогнать hnm!

    Размер - 49к.

4) [KOTHD](https://github.com/abdoelsayed2016/KOHTD)

    Датасет с множеством отсканированных рукописных текстов. 
    
    **Рукописный. Казахский язык. Сканы.**
    
    Качество высокое (проблемные сэмплы не найдены), размер - около 115k.

5) [PDFA (pdfa-eng-wds)](https://huggingface.co/datasets/pixparse/pdfa-eng-wds)

    Датасет из сканов .pdf файлов. 

    **Печатный. Английский язык. Сканы.**

    Качество высокое. Размер - 247k.

6) [SynthRu (synthetic_hkr)](https://huggingface.co/datasets/nastyboget/synthetic_hkr/tree/main)

    Датасет рукописной синтетики с различными шрифтами.

    **Рукописный. Русский язык. Синтетика.**

    Качество выше среднего (довольно длинные примеры, что немного может путать модель, рассчитанную на одно слово). Размер - 300k.

7) WikiKZ - Датасет из скачанных казахских документов. 

    Распаршенные документы с сайтов казахского нац банка и юридического сайта.

    **Печатный. Казахский. Сканы.**

    Качество выше среднего, размер - 102k.

8) [HWCYR (cyrillic-handwriting-dataset)](https://www.kaggle.com/datasets/constantinwerner/cyrillic-handwriting-dataset) (Ещё не вошёл в датасет)

    Фотографии русских рукописных слов из тетрадок и документов.

    **Рукописный. Русский язык. Фото.**

    Качество требуется проверить, размер уточняется.

### Версионирование

0) Неактуальные датасеты - `v1` (содержал в себе датасеты 1, 4, 5, 6)

1) На данный момент актуальна версия датасета `v2`, которая содержит все датасеты 1-7. 

2) Следующая версия `v3` будет содержать также новый датасет под номером 8.

Основное версионирование датасетов будет происходить по ключевым изменениям. В ключевые изменения относим:

* Добавление/удаление некоторого количества данных из датасета (пока мы берём не всё, ибо некоторые датасеты очень большие)
* Добавление/удаление нового датасета или нескольких датасетов
* Полную предобработку всего OCR датасета с помощью эвристики (например, фильтрации и т.д.), которая меняет пайплайн взаимодействия с моделингом (например, для реализации нового вокаба потребуется новая сортировка, может потребоваться полный пересбор датасетов)

В минорные и ответвления относим вырезки датасетов, фильтрацию одного конкретного датасета.

Также локально храним датасеты для каждого отдельного набора данных (в формате `ocr_<dataset_name>.parquet`). Это промежуточный шаг, необходимый для сохранения всех базовых состояний датасета (без предобработок)

### Текущая актуальная версия 
Для первого обучения используем уменьшенную сбалансированную версию актуального датасета `v2` - `ocr_dataset_v2_50k.parquet`. 

Данный датасет содержит в себе по 50 тысяч примеров из каждого набора данных, что улучшает равномерное влияние каждого из них на обучение.

### Скачивание датасетов

Все датасеты хранятся в репозитории hugging_face и доступны через простой запуск dvc

```bash
dvc get https://huggingface.co/NerfmanOriginal/OCR-data-900k ocr_dataset.zip
dvc get https://huggingface.co/NerfmanOriginal/OCR-data-900k ocr_dataset_v2_50k.parquet
dvc get https://huggingface.co/NerfmanOriginal/OCR-data-900k ocr_dataset_v2.parquet
```

**Важно**: этот способ подтянет датасет полностью, со всеми изображениями (около 12 гб памяти сам архив, на распаковку ещё 12).

Если был скачан датасет с изображениями (`ocr_dataset.zip`), необходимо распаковать данные любым удобным образом. Можно сделать просто 7z или unzip.

Конечный результат - папка `ocr_dataset/` cо всеми изображениями и файлы `.parquet` с аннотациями.

### Формат аннотаций

Аннотации хранятся в `.parquet` файлах и содержат несколько колонок - `image_path`, `text`, `source`. 
Они отвечают за путь до файла, текст ground_true и источник датасета (чтобы быстро ориентироваться в том, из какого источника это взято)

### Форматирование датасетов и фильтрации

На данный момент тетрадь для предобработки датасетов в разработке.

Планируемые для реализации элементы:

* Фильтрация всех данных по длине выходного окна.
* Фильтрация или замена всех символов, не попадающих в вокаб.
* Отсеивание плохих результатов по Hard Negative Mining.
* Возможность выбирать желаемый размер каждого набора данных при вхождении в итоговый датасет.

**Важно:** все эти обработки пока планируется реализовать для файлов с отдельными наборами данных. 